
<!-- PROJECT LOGO -->
<br />
<div align="center">
  <h1 align="center">Fine Tuning Llama 2 Using LoRA <br/> Scientific Questions Answering </h1>
</div>

<!-- ABOUT THE PROJECT -->
## About The Project

This project showcases a fine-tuned Llama 2 model utilizing LoRA (Low Rank Adaptation). This approach accelerates the fine-tuning process without losing accuracy.



Model use dataset of Scientific Question Answers: [SciQ](https://www.kaggle.com/datasets/thedevastator/sciq-a-dataset-for-science-question-answering)

Llama 2 Model: [Llama 2 Model](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf)

Guide for fine tuning Llama 2 using LoRA: [deci.ai](https://deci.ai/blog/fine-tune-llama-2-with-lora-for-question-answering/)



## COMING SOON ...

<!-- ## Installation

1. Clone the repository
   ```sh
   git clone https://github.com/trungmac07/Cpp_Problem_Solving_Llama_2.git
   ```
2. Install related packages
    ```sh
    pip install -r requirements.txt
    ```

## Usage
### 1. Training

Because of the limitations of devices in training, fine tuning the model takes a long time. To manage this, train the model in parts, saving and loading it between sessions. Look for two training guides in the "notebook" folder.

- `begin_fine_tuning.ipynb`: notebook for training from beginning
- `continue_fine_tuning.ipynb`: notebook for continuing training after 

### 2. GUI application
Execute the command below. The application will open with a white box for entering review sentences. Click the button to analyze the sentence and view the prediction results with percentages.

```sh
   python.exe Sentiment_Analysis.py
```

## Contact
- Email: mttrung16@gmail.com  -->








